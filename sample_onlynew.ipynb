{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from knsmote import KMeansSMOTE\n",
    "# from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "from data import get_data\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X, y, X_feature_names = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({4: 22503, 3: 8570, 0: 306, 2: 148, 1: 29})\n",
      "Resampled class distribution: Counter({4: 37509, 3: 23578, 0: 15309, 2: 15152, 1: 15031})\n",
      "Extracting synthetic samples for class 0...\n",
      "Extracting synthetic samples for class 1...\n",
      "Extracting synthetic samples for class 2...\n",
      "Extracting synthetic samples for class 3...\n",
      "Extracting synthetic samples for class 4...\n"
     ]
    }
   ],
   "source": [
    "# Convert X to CSR if it's COO (for indexing)\n",
    "if sparse.isspmatrix_coo(X):\n",
    "    X = X.tocsr()\n",
    "\n",
    "# Print original class distribution\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "original_sample_count = len(y)\n",
    "\n",
    "# Define target counts for each class\n",
    "samples_per_class = 15000\n",
    "unique_classes = np.unique(y)\n",
    "\n",
    "# Calculate required number of samples for each class\n",
    "# Add original count to target count so we'll have enough after removing original samples\n",
    "sampling_strategy = {\n",
    "    label: samples_per_class + np.sum(y == label)\n",
    "    for label in unique_classes\n",
    "}\n",
    "\n",
    "# Configure KMeansSMOTE\n",
    "smote = KMeansSMOTE(\n",
    "    kmeans_estimator=MiniBatchKMeans(n_clusters=20, n_init=1, random_state=0),\n",
    "    cluster_balance_threshold=0.001,\n",
    "    sampling_strategy=sampling_strategy,  # Use our custom sampling strategy\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply KMeansSMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert to CSR if needed\n",
    "if sparse.isspmatrix_coo(X_resampled):\n",
    "    X_resampled = X_resampled.tocsr()\n",
    "\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "# Create empty arrays to hold our purely synthetic samples\n",
    "synthetic_X = []\n",
    "synthetic_y = []\n",
    "\n",
    "# For each class, extract exactly 15k synthetic samples\n",
    "for class_label in unique_classes:\n",
    "    print(f\"Extracting synthetic samples for class {class_label}...\")\n",
    "    \n",
    "    # Original samples of this class\n",
    "    orig_class_count = np.sum(y == class_label)\n",
    "    \n",
    "    # Find synthetic samples for this class\n",
    "    # They start after all original samples\n",
    "    synthetic_class_indices = np.where(\n",
    "        (y_resampled == class_label) & \n",
    "        (np.arange(len(y_resampled)) >= original_sample_count)\n",
    "    )[0]\n",
    "    \n",
    "    # If we didn't generate enough synthetic samples\n",
    "    if len(synthetic_class_indices) < samples_per_class:\n",
    "        print(f\"Warning: Only generated {len(synthetic_class_indices)} synthetic samples for class {class_label}\")\n",
    "        \n",
    "        # If we have insufficient synthetic samples, we'll need to duplicate some\n",
    "        needed = samples_per_class - len(synthetic_class_indices)\n",
    "        duplicate_indices = np.random.choice(synthetic_class_indices, size=needed, replace=True)\n",
    "        \n",
    "        # Combine original synthetic indices with duplicates\n",
    "        synthetic_class_indices = np.concatenate([synthetic_class_indices, duplicate_indices])\n",
    "    \n",
    "    # Take exactly 15k synthetic samples\n",
    "    synthetic_class_indices = synthetic_class_indices[:samples_per_class]\n",
    "    \n",
    "    # Extract the synthetic samples\n",
    "    if sparse.issparse(X_resampled):\n",
    "        class_synthetic_X = X_resampled[synthetic_class_indices].toarray()\n",
    "    else:\n",
    "        class_synthetic_X = X_resampled[synthetic_class_indices]\n",
    "        \n",
    "    class_synthetic_y = y_resampled[synthetic_class_indices]\n",
    "    \n",
    "    # Add to our collection\n",
    "    synthetic_X.append(class_synthetic_X)\n",
    "    synthetic_y.append(class_synthetic_y)\n",
    "\n",
    "# Combine all synthetic samples\n",
    "all_synthetic_X = np.vstack(synthetic_X)\n",
    "all_synthetic_y = np.concatenate(synthetic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 5287)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_synthetic_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_withh5py(X_res, y_res):\n",
    "    # Convert X to sparse CSR if it's dense\n",
    "    if not sparse.issparse(X_res):\n",
    "        X_res = sparse.csr_matrix(X_res)\n",
    "\n",
    "    with h5py.File(\"output/features4ausw4linearsvc_trainsampledonlynew.h5\", \"w\") as f:\n",
    "        # Save sparse matrix X as a compressed dataset\n",
    "        f.create_dataset(\"X_data\", data=X_res.data)\n",
    "        f.create_dataset(\"X_indices\", data=X_res.indices)\n",
    "        f.create_dataset(\"X_indptr\", data=X_res.indptr)\n",
    "        f.create_dataset(\"X_shape\", data=X_res.shape)\n",
    "\n",
    "        # Save y as a dense dataset\n",
    "        f.create_dataset(\"y\", data=y_res)\n",
    "\n",
    "save_data_withh5py(all_synthetic_X, all_synthetic_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
